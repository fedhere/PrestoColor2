{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask+Jupyter@NERSC Best Practices: Introduction\n",
    "\n",
    "This notebook outlines current best practices for using\n",
    "[Dask Distributed](https://distributed.dask.org/en/stable/)\n",
    "to do parallel computations on \n",
    "[Cori's](https://docs.nersc.gov/systems/cori/)\n",
    "compute nodes, using NERSC's\n",
    "[Jupyter service.](https://docs.nersc.gov/services/jupyter/)\n",
    "\n",
    "## Best Practice: Use a Shifter Image to Run a Dask Cluster and Kernel\n",
    "\n",
    "For your own Dask+Jupyter@NERSC workflows, we recommend that you set up a\n",
    "[Shifter](https://docs.nersc.gov/development/shifter/overview/)\n",
    "image that you can use to run both a Jupyter\n",
    "[kernel](https://jupyter.readthedocs.io/en/latest/projects/kernels.html)\n",
    "and a Dask cluster (Dask scheduler, dashboard, and workers).\n",
    "This demonstration notebook uses such a Shifter image for both kernel and cluster.\n",
    "\n",
    "Using the same Shifter image for both the kernel and the cluster ensures that both have the same Python interpreter, packages, and package versions.\n",
    "Using Shifter to launch the Dask cluster helps it start up much much faster than otherwise.\n",
    "\n",
    "## Best Practice: Use dask-mpi to Launch the Dask Cluster\n",
    "\n",
    "If you want to run a Dask cluster on Cori compute nodes, we recommend using \n",
    "[dask-mpi](http://mpi.dask.org/en/latest/).\n",
    "While it does not leverage MPI for communiction during computation, we have found that using MPI to start Dask cluster processes is faster and less hassle than other options.\n",
    "\n",
    "## Best Practice: Use start-dask-cluster to Simplify Things\n",
    "\n",
    "The fastest way to get a Dask cluster running on Cori compute nodes is through Cori's interactive QOS.\n",
    "But setting up the job with all the right flags is a bit complicated.\n",
    "To simplify the process on Cori, we have created a software module that provides a wrapper script called \"start-dask-cluster.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Start the Dask Cluster\n",
    "\n",
    "Open a Jupyter terminal tab (or regular Cori terminal), and run these commands:\n",
    "\n",
    "    cd $SCRATCH\n",
    "    module load nersc-dask\n",
    "    start-dask-mpi --ntasks=256 --image=stephey/nersc-dask-example:v0.6.0\n",
    "\n",
    "The script will not start unless you are somewhere in your \\\\$SCRATCH directory.\n",
    "Any path under \\\\$SCRATCH will be fine.\n",
    "\n",
    "## What This Does\n",
    "\n",
    "The wrapper runs `salloc` to launch a Dask cluster for you in the interactive queue, using a Shifter image and dask-mpi.\n",
    "Just before the job is submitted, you should see something like:\n",
    "\n",
    "    OMP_NUM_THREADS     1\n",
    "    PYTHONUNBUFFERED    1\n",
    "    salloc\n",
    "        --image=stephey/nersc-dask-example:v0.6.0\n",
    "        --nodes=8\n",
    "        --ntasks=256\n",
    "        --cpus-per-task=2\n",
    "        --time=30\n",
    "        --constraint=haswell\n",
    "        --qos=interactive\n",
    "        srun -u shifter\n",
    "            dask-mpi\n",
    "                --scheduler-file=scheduler.json\n",
    "                --dashboard-address=0\n",
    "                --nthreads=1\n",
    "                --memory-limit='auto'\n",
    "                --no-nanny\n",
    "                --local-directory=/tmp\n",
    "                \n",
    "The script formatted that job request for you, and along the way figured out how many nodes you need to run the job, did a few checks, and set a few sensible defaults for you as well.\n",
    "You can change many of these options.\n",
    "See\n",
    "\n",
    "    start-dask-mpi.py --help\n",
    "\n",
    "for more information.\n",
    "\n",
    "Wait for the job to start (you may need to re-run the command if the interactive queue is really busy).  There will be a lot of output from the cluster when it starts up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Some Preparations\n",
    "\n",
    "When your Dask cluster starts it will drop a scheduler file to the path where you ran start-dask-mpi.\n",
    "Your notebook needs to pick this up so it knows how to connect to the scheduler.\n",
    "In the above example we had you `cd $SCRATCH` so the scheduler file will be there.\n",
    "\n",
    "We also need to set up a link for the Dask dashboard to be visible.\n",
    "This is optional but the dashboard is extremely helpful for understanding what your cluster and your workload is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "scheduler_file = os.path.join(os.environ[\"SCRATCH\"], \"scheduler.json\")\n",
    "dask.config.config[\"distributed\"][\"dashboard\"][\"link\"] = \"{JUPYTERHUB_SERVICE_PREFIX}proxy/{host}:{port}/status\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Connect to the Scheduler\n",
    "\n",
    "The client is an object that manages communication with your cluster.\n",
    "Initialize a client by passing it the path to the scheduler file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(scheduler_file=scheduler_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the client object results in a widget with some information in it.\n",
    "Click the dashboard link, which will start up another tab with the dashboard in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Set Up a Parallel Calculation\n",
    "\n",
    "All we will do here is use a crude Monte Carlo calculation to estimate the value of $\\pi$.\n",
    "We use the dart-board method:\n",
    "\n",
    "- Take the first quadrant of the unit square, and within that the first quadrant of the unit circle.\n",
    "- Simulate throwing darts randomly at the square with a uniformly random distribution in x and y.\n",
    "- Take the ratio of darts landing within the first quadrant of the unit circle to all darts thrown.\n",
    "- Multiply the result by 4 to approximage $\\pi$.\n",
    "- The more darts thrown, the more precise the result.\n",
    "\n",
    "The method we are using (Monte Carlo) parallelizes trivially.\n",
    "You can combine the results of multiple random trials to get a more precise answer.\n",
    "Here's the function that implements the dart board simulation.\n",
    "Nothing fancy here, it just returns the number of hits inside the circle, and thet total number of throws (count).\n",
    "Each run should get a unique seed to try to help ensure all the trials are \"independent.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def simulate(seed, count=100):\n",
    "    np.random.seed(seed)\n",
    "    xy = np.random.uniform(size=(count, 2))\n",
    "    return ((xy * xy).sum(1) < 1.0).sum(), count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Use the map() Function to Distribute Tasks to Workers\n",
    "\n",
    "We're going to ask for some very large number of total throws and chop them up into a smaller number of tasks.\n",
    "The scheduler figures out how to distribute all the work for us.\n",
    "Typical scheduling overhead is 1 millisecond per task.\n",
    "When you actually start the calculation further down it may thus take a little time for the dashboard to react."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 100000000000\n",
    "tasks = 10000\n",
    "count = total // tasks\n",
    "futures = client.map(simulate, list(9876543 + np.arange(tasks, dtype=int)), count=count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Use the submit() Function to Get the Answer\n",
    "\n",
    "The map() function and a lot of other functions from Dask return \"futures\" which are placeholders for computation.\n",
    "These may be not done yet, or they may be done, and you need to do something to realize them into their final result form.\n",
    "We'll submit a final reducer task that computes the sum of hits and divides it by the sum of all simulated throws.\n",
    "Here's the reducer function, with a multiplication by 4 to get to our estimate of $\\pi$.\n",
    "It takes in the futures and acts on them like they're real results.\n",
    "That's the magic of Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(results):\n",
    "    total_hits = 0\n",
    "    total_count = 0\n",
    "    for hits, count in results:\n",
    "        total_hits += hits\n",
    "        total_count += count\n",
    "    return 4.0 * total_hits / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we submit the reducer.\n",
    "The whole calculation with 256 processes takes under a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "client.submit(reduce, futures).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to shut down the cluster, you can just stop the job either by using `scancel` from another window, or keyboard interrupt (CTRL-C).\n",
    "The `scheduler.json` file may be cleaned up by dask-mpi.\n",
    "If it is not, then next time you run the wrapper script, it will be pre-emptively deleted.\n",
    "Otherwise your workers may try to connect to a scheduler that is no longer there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Review\n",
    "\n",
    "Here is a summary of what we did:\n",
    "\n",
    "- Used a Shifter image to run both the kernel for this notebook and a Dask cluster on compute nodes\n",
    "- Used dask-mpi to launch the Dask cluster on the compute nodes \"in\" the Shifter image\n",
    "- Simplified launching the Dask cluster by using a helpful wrapper script, start-dask-mpi.py\n",
    "- Ran a simple parallel Monte Carlo estimation of $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "\n",
    "- [Dask](https://docs.dask.org/en/stable/)\n",
    "- [Dask Distributed](https://distributed.dask.org/en/stable/)\n",
    "- [Dask-MPI](http://mpi.dask.org/en/latest/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
